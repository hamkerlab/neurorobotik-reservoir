{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward-driven learning of recurrent dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ANNarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Reward-modulated recurrent network based on:\n",
    "\n",
    "> Miconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ANNarchy as ann\n",
    "ann.clear()\n",
    "ann.setup(dt=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron in the reservoir follows the following equations:\n",
    "\n",
    "$$\n",
    "    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    r(t) = \\tanh(x(t))\n",
    "$$\n",
    "\n",
    "where $\\xi(t)$ is a random perturbation at 3 Hz, with an amplitude randomly sampled between $-A$ and $+A$.\n",
    "\n",
    "We additionally keep track of the mean firing rate with a sliding average:\n",
    "\n",
    "$$\n",
    "    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n",
    "$$\n",
    "\n",
    "The three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = ann.Neuron(\n",
    "    parameters = \"\"\"\n",
    "        tau = 30.0 : population # Time constant\n",
    "        constant = 0.0 # The four first neurons have constant rates\n",
    "        alpha = 0.05 : population # To compute the sliding mean\n",
    "        f = 3.0 : population # Frequency of the perturbation\n",
    "        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n",
    "    \"\"\",\n",
    "    equations=\"\"\"\n",
    "        # Perturbation\n",
    "        perturbation = if Uniform(0.0, 1.0) < f/1000.: 1.0 else: 0.0 \n",
    "        noise = if perturbation > 0.5: A * Uniform(-1.0, 1.0) else: 0.0\n",
    "\n",
    "        # ODE for x\n",
    "        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n",
    "\n",
    "        # Output r\n",
    "        rprev = r # store r at previous time step\n",
    "        r = if constant == 0.0: tanh(x) else: tanh(constant)\n",
    "\n",
    "        # Sliding mean\n",
    "        delta_x = x - x_mean\n",
    "        x_mean = alpha * x_mean + (1 - alpha) * x\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rule is defined by a trace $e_{i, j}(t)$ for each synapse $i \\rightarrow j$ incremented at each time step with:\n",
    "\n",
    "$$\n",
    "    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n",
    "$$\n",
    "\n",
    "At the end $T$ of a trial, the reward $R$ is delivered and all weights are updated using:\n",
    "\n",
    "$$\n",
    "    \\Delta w_{i, j} = \\eta \\,  e_{i, j}(T) \\, |R_\\text{mean}| \\,  (R - R_\\text{mean})\n",
    "$$\n",
    "\n",
    "where $R_\\text{mean}$ is the mean reward for the task. Here the reward is defined as the opposite of the prediction error.\n",
    "\n",
    "All traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\n",
    "\n",
    "As ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean `learning_phase` which performs trace integration when 0, weight update when 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse = ann.Synapse(\n",
    "    parameters=\"\"\"\n",
    "        eta = 0.5 : projection # Learning rate\n",
    "        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n",
    "        reward = 0.0 : projection # Reward received\n",
    "        mean_reward = 0.0 : projection # Mean Reward received\n",
    "        max_weight_change = 0.0003 : projection # Clip the weight changes\n",
    "    \"\"\",\n",
    "    equations=\"\"\"\n",
    "        # Trace\n",
    "        trace += if learning_phase < 0.5:\n",
    "                    power(pre.rprev * (post.delta_x), 3)\n",
    "                 else:\n",
    "                    0.0\n",
    "\n",
    "        # Weight update only at the end of the trial\n",
    "        delta_w = if learning_phase > 0.5:\n",
    "                eta * trace * fabs(mean_reward) * (reward - mean_reward)\n",
    "             else:\n",
    "                 0.0 : min=-max_weight_change, max=max_weight_change\n",
    "        w += delta_w\n",
    "        \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which having constant rates to serve as biases for the other neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input population\n",
    "inp = ann.Population(2, ann.Neuron(parameters=\"r=0.0\"))\n",
    "\n",
    "# Recurrent population\n",
    "N = 200\n",
    "pop = ann.Population(N, neuron)\n",
    "\n",
    "# Biases\n",
    "pop[0].constant = 1.0\n",
    "pop[1].constant = 1.0\n",
    "pop[2].constant = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input weights are uniformly distributed between -1 and 1.\n",
    "\n",
    "The recurrent weights are normally distributed, with a coupling strength of $g=1.5$ (edge of chaos). In the original paper, the projection is fully connected (but self-connections are avoided). Using a sparse (0.1) connectivity matrix leads to similar results and is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input weights\n",
    "Wi = ann.Projection(inp, pop, 'in')\n",
    "Wi.connect_all_to_all(weights=ann.Uniform(-1.0, 1.0))\n",
    "\n",
    "# Recurrent weights\n",
    "g = 1.5\n",
    "sparseness = 0.1\n",
    "\n",
    "Wrec = ann.Projection(pop, pop, 'exc', synapse)\n",
    "if sparseness == 1.0:\n",
    "    Wrec.connect_all_to_all(weights=ann.Normal(0., g/np.sqrt(N)))\n",
    "else:\n",
    "    Wrec.connect_fixed_probability(probability=sparseness, weights=ann.Normal(0., g/np.sqrt(sparseness*N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the reservoir is chosen to be the neuron of index 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_NEURON = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We record the rates inside the reservoir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ann.Monitor(pop, ['r'], start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the DNMS (delayed non-match-to-sample) task used by Miconi in the paper. A stimulus (`A` or `B`) is presented for `d_stim = 200` ms followed by a delay period `d_delay = 200` ms. A second stimulus (`A` or `B`) is presented again. After another delay period, the activity of the output neuron is recorded for `d_response = 200`ms. If the two stimulus were identical, the mean activity of the output neuron during the response period should be close to -1, otherwise it should be close to +1. \n",
    "\n",
    "Parameters defining the duration of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durations\n",
    "d_stim = 200\n",
    "d_delay= 200\n",
    "d_response = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a DNMS trial (AA, AB, BA, BB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnms_trial(\n",
    "        input:list, # List of integers (0 or 1) for the inputs: AA, AB, BA, BB\n",
    "        target:float, # Target activity for the output neuron (+- 0.98)\n",
    "        R_mean:float, # Mean reward \n",
    "        learning:bool = True, # Learning enabled\n",
    "        record:bool = False, # Recording enabled\n",
    "        perturbation:bool = True # Perturbations enabled\n",
    "    ):\n",
    "\n",
    "    # Switch off perturbations if needed\n",
    "    if not perturbation:\n",
    "        old_A = pop.A\n",
    "        pop.A = 0.0\n",
    "\n",
    "    # Reinitialize network\n",
    "    pop.x = ann.Uniform(-0.1, 0.1).get_values(N)\n",
    "    pop.r = np.tanh(pop.x)\n",
    "    pop[0].r = np.tanh(1.0)\n",
    "    pop[1].r = np.tanh(1.0)\n",
    "    pop[2].r = np.tanh(-1.0)\n",
    "\n",
    "    if record: m.resume()\n",
    "\n",
    "    # First input\n",
    "    inp[input[0]].r = 1.0\n",
    "    ann.simulate(d_stim)\n",
    "    \n",
    "    # Delay\n",
    "    inp.r = 0.0\n",
    "    ann.simulate(d_delay)\n",
    "    \n",
    "    # Second input\n",
    "    inp[input[1]].r = 1.0\n",
    "    ann.simulate(d_stim)\n",
    "    \n",
    "    # Delay\n",
    "    inp.r = 0.0\n",
    "    ann.simulate(d_delay)\n",
    "    \n",
    "    # Response\n",
    "    if not record: m.resume()\n",
    "    inp.r = 0.0\n",
    "    ann.simulate(d_response)\n",
    "    \n",
    "    # Read the output\n",
    "    m.pause()\n",
    "    recordings = m.get('r')\n",
    "    \n",
    "    # Response is over the last 200 ms\n",
    "    output = recordings[-int(d_response):, OUTPUT_NEURON] # neuron 100 over the last 200 ms\n",
    "    \n",
    "    # Compute the reward as the opposite of the absolute error\n",
    "    reward = - np.mean(np.abs(target - output))\n",
    "    \n",
    "    # The first 25 trial do not learn, to let R_mean get realistic values\n",
    "    if learning:\n",
    "\n",
    "        # Apply the learning rule\n",
    "        Wrec.learning_phase = 1.0\n",
    "        Wrec.reward = reward\n",
    "        Wrec.mean_reward = R_mean\n",
    "\n",
    "        # Learn for one step\n",
    "        ann.step()\n",
    "        \n",
    "        # Reset the traces\n",
    "        Wrec.learning_phase = 0.0\n",
    "        Wrec.trace = 0.0\n",
    "        _ = m.get() # to flush the recording of the last step\n",
    "\n",
    "    # Switch back on perturbations if needed\n",
    "    if not perturbation:\n",
    "        pop.A = old_A\n",
    "\n",
    "    return recordings, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the activity of the output neuron during the first four trials without learning (AA, AB, BA and BB). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the four different trials successively\n",
    "initialAA, errorAA = dnms_trial(input=[0, 0], target=-0.98, R_mean=0.0, learning=False, record=True, perturbation=True)\n",
    "initialAB, errorAB = dnms_trial(input=[0, 1], target=+0.98, R_mean=0.0, learning=False, record=True, perturbation=True)\n",
    "initialBA, errorBA = dnms_trial(input=[1, 0], target=+0.98, R_mean=0.0, learning=False, record=True, perturbation=True)\n",
    "initialBB, errorBB = dnms_trial(input=[1, 1], target=-0.98, R_mean=0.0, learning=False, record=True, perturbation=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = plt.subplot(221)\n",
    "ax.plot(initialAA[:, OUTPUT_NEURON], lw=3)\n",
    "ax.plot(initialAA[:, OUTPUT_NEURON + 1], lw=1)\n",
    "ax.plot(initialAA[:, OUTPUT_NEURON + 2], lw=1)\n",
    "ax.plot(initialAA[:, OUTPUT_NEURON + 3], lw=1)\n",
    "ax.set_ylim((-1., 1.))\n",
    "ax.set_title('Output AA -1')\n",
    "ax = plt.subplot(222)\n",
    "ax.plot(initialBA[:, OUTPUT_NEURON], lw=3)\n",
    "ax.plot(initialBA[:, OUTPUT_NEURON + 1], lw=1)\n",
    "ax.plot(initialBA[:, OUTPUT_NEURON + 2], lw=1)\n",
    "ax.plot(initialBA[:, OUTPUT_NEURON + 3], lw=1)\n",
    "ax.set_ylim((-1., 1.))\n",
    "ax.set_title('Output BA +1')\n",
    "ax = plt.subplot(223)\n",
    "ax.plot(initialAB[:, OUTPUT_NEURON], lw=3)\n",
    "ax.plot(initialAB[:, OUTPUT_NEURON + 1], lw=1)\n",
    "ax.plot(initialAB[:, OUTPUT_NEURON + 2], lw=1)\n",
    "ax.plot(initialAB[:, OUTPUT_NEURON + 3], lw=1)\n",
    "ax.set_ylim((-1., 1.))\n",
    "ax.set_title('Output AB +1')\n",
    "ax = plt.subplot(224)\n",
    "ax.plot(initialBB[:, OUTPUT_NEURON], lw=3)\n",
    "ax.plot(initialBB[:, OUTPUT_NEURON + 1], lw=1)\n",
    "ax.plot(initialBB[:, OUTPUT_NEURON + 2], lw=1)\n",
    "ax.plot(initialBB[:, OUTPUT_NEURON + 3], lw=1)\n",
    "ax.set_ylim((-1., 1.))\n",
    "ax.set_title('Output BB -1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the initial weights, the activity of the output neuron during the last 200ms is different from the desired values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Perform the simulation for 5000 groups of learning trials (AA, AB. BA and BB). Beware, this can take 15 to 20 minutes.\n",
    "\n",
    "You will need to update the mean reward `R_mean` for each of the four trials separately (AA, AB, BA, BB). The mean reward can be computed as a simple sliding average of the reward:\n",
    "\n",
    "$$R_\\text{mean}(\\text{AA}) = \\alpha \\, R_\\text{mean}(\\text{AA}) + (1 - \\alpha) \\ r$$\n",
    "$$R_\\text{mean}(\\text{AB}) = \\alpha \\, R_\\text{mean}(\\text{AB}) + (1 - \\alpha) \\ r$$\n",
    "$$R_\\text{mean}(\\text{BA}) = \\alpha \\, R_\\text{mean}(\\text{BA}) + (1 - \\alpha) \\ r$$\n",
    "$$R_\\text{mean}(\\text{BB}) = \\alpha \\, R_\\text{mean}(\\text{BB}) + (1 - \\alpha) \\ r$$\n",
    "\n",
    "where $r$ is the reward received at the end of each trial and $\\alpha=0.75$. The mean reward can be initialized to -1. \n",
    "\n",
    "The first 25 trials should be made without learning, so that `R_mean`has time to reach a meaningful value. \n",
    "\n",
    "Record the rewards received during each trial and make a plot. Is learning fast or slow? Stable or unstable? Compare with FORCE learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Visualize the activity of the output neuron during the four trials after training, without learning nor perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d24234067c217f49dc985cbc60012ce72928059d528f330ba9cb23ce737906d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
