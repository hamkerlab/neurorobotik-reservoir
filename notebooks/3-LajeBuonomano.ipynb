{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bfe5e9ff-4b71-47ba-974b-be3dad2af804"
   },
   "source": [
    "# Taming chaos in reservoir networks with FORCE learning\n",
    "\n",
    "This notebook shows a reimplementation of the reservoir network using FORCE learning of the recurrent weights by Laje & Buonomano:\n",
    "\n",
    "> Laje, R., and Buonomano, D.V. (2013). Robust timing and motor patterns by taming chaos in recurrent neural networks. Nature Neuroscience 16, 925â€“933. doi:10.1038/nn.3405.\n",
    "\n",
    "The goal is to better understand how the network works, what are the limits, in order to be able to apply it to a robotic task later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the class representing the network, based on the [Re]implementation published here:\n",
    "\n",
    "> Vitay, J. (2016). [Re] Robust Timing And Motor Patterns By Taming Chaos In Recurrent Neural Networks. ReScience 2. doi:10.5281/zenodo.159545\n",
    "\n",
    "The network can take `Ni` inputs, has `N` recurrent neurons and `No` output neurons. Each recurrent neuron follows the standard RC equation:\n",
    "\n",
    "$$\n",
    "    \\tau \\cdot \\frac{d x_i(t)}{dt} + x_i(t) = \\sum_{j=1}^{N_i} W_{ij}^{in} \\cdot I_j(t) + \\sum_{j=1}^{N} W_{ij}^{rec} \\cdot r_j(t) + I^{noise}_i(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    r_i(t) = \\tanh x_i(t) \n",
    "$$\n",
    "\n",
    "The input weights $W^{in}$ are randomly taken from the standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "The recurrent weights $W^{rec}$ are sparse (two neurons have a probability `pc=0.1` to be connected) and initialized using the normal distribution $\\mathcal{N}(0, \\frac{g^2}{pc \\, N})$, where $g$ is the scaling factor determining the dynamics of the network. In the implementation, the sparseness of the connectivity matrix is realized using a fixed connectivity mask that multiplies the non existing weights with 0.\n",
    "\n",
    "The readout neurons simply multiply the activity in the reservoir with the readout weights $W^{out}$ (initially sampled from $\\mathcal{N}(0, \\frac{1}{N})$) at each time step:\n",
    "\n",
    "$$\n",
    "    z_i(t) = \\sum_{j=1}^{N} W_{ij}^{out} \\cdot r_j(t)\n",
    "$$\n",
    "\n",
    "The method `step()` implements these equations, based on an input `I` which has to be of shape `(Ni, 1)` (vector notation). Note that the Euler numerical method is used for the ordinary differential equation. Additive noise can be switched on and off using the corresponding flag.\n",
    "\n",
    "Both recurrent and readout weights can be learned using the **recursive least squares** (RLS) algorithm for FORCE learning. In Laje & Buonomano, the recurrent weights are trained first to consistently reproduce a desired trajectory $r^*_i(t)$. Afterwards, the readout weights are trained to reproduce any kind of signal.\n",
    "\n",
    "To train the recurrent weights, a first trial is made and the activity of all recurrent neurons is recorded for a fixed duration. This allows later to compute an error signal for all recurrent neurons:\n",
    "\n",
    "$$\n",
    "    e_i(t) = r^*_i(t) - r_i(t)\n",
    "$$\n",
    "\n",
    "At each time step, the recurrent weighs are updated depending on the error and the **inverse of the information matrix** $P^i$ of the recurrent inputs $\\mathcal{B}(i)$ to the neuron $i$. \n",
    "\n",
    "There is one matrix $P^i$ per (plastic) neuron in the reservoir, which is updated at every time step. To reduce the computational burden, only a proportion `P_plastic` of the neurons are plastic and actually learn their weights (in the implementation, the first `N_plastic` neurons of the reservoir). Furthermore, the recurrent weight matrix is **sparse**, i.e. only `pc` percents of the possible connections are created, so each neuron receives around `pc * N` connection. The matrix $P$ for each neuron is fortunately much smaller than $N times N$.\n",
    "\n",
    "The RLS learning rule is implemented in the `train_recurrent()` method, which takes a target signal of shape `(N, 1)` as input. A similar procedure for the readout weights is implemented in `train_readout()`. \n",
    "\n",
    "**Q:** Read and understand the  class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3830b709-043b-46b9-bcb9-8b9df8c80d53"
   },
   "outputs": [],
   "source": [
    "class LajeBuonomano(object):\n",
    "    \"\"\"\n",
    "    Class implementing a recurrent network with readout weights and RLS learning rules for both the recurrent and readout weights.\n",
    "\n",
    "    Args:\n",
    "        Ni: Number of input neurons\n",
    "        N: Number of recurrent neurons\n",
    "        No: Number of read-out neurons\n",
    "        tau: Time constant of the neurons\n",
    "        g: Synaptic strength scaling\n",
    "        pc: Connection probability\n",
    "        Io: Noise variance\n",
    "        delta: Learning rate\n",
    "        P_plastic: Percentage of neurons receiving plastic synapses\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 Ni:int = 2,\n",
    "                 N:int = 800, \n",
    "                 No:int = 1, \n",
    "                 tau:float = 10.0, \n",
    "                 g:float = 1.5, \n",
    "                 pc:float = 0.1, \n",
    "                 Io:float = 0.01, \n",
    "                 delta:float = 1.0, \n",
    "                 P_plastic:float = 0.6):\n",
    "        \n",
    "        # Copy the parameters\n",
    "        self.Ni = Ni\n",
    "        self.N = N\n",
    "        self.No = No\n",
    "        self.tau = tau\n",
    "        self.g = g\n",
    "        self.pc = pc\n",
    "        self.Io = Io\n",
    "        self.P_plastic = P_plastic\n",
    "        self.N_plastic = int(self.P_plastic*self.N) # Number of plastic cells = 480\n",
    "        self.delta = delta\n",
    "\n",
    "        # Input\n",
    "        self.I = np.zeros((self.Ni, 1))\n",
    "\n",
    "        # Recurrent population\n",
    "        self.x = np.zeros((self.N, 1))\n",
    "        self.r = np.tanh(self.x)\n",
    "\n",
    "        # Read-out population\n",
    "        self.z = np.zeros((self.No, 1))\n",
    "\n",
    "        # Weights between the input and recurrent units\n",
    "        self.W_in = np.random.randn(self.N, self.Ni)\n",
    "\n",
    "        # Weights between the recurrent units\n",
    "        self.W_rec = (np.random.randn(self.N, self.N) * self.g/np.sqrt(self.pc*self.N))\n",
    "\n",
    "        # The connection pattern is sparse with p=0.1\n",
    "        connectivity_mask = np.random.binomial(1, self.pc, (self.N, self.N))\n",
    "        connectivity_mask[np.diag_indices(self.N)] = 0\n",
    "        self.W_rec *= connectivity_mask\n",
    "\n",
    "        # Store the pre-synaptic neurons to each plastic neuron\n",
    "        self.W_plastic = [list(np.nonzero(connectivity_mask[i, :])[0]) for i in range(self.N_plastic)]\n",
    "\n",
    "        # Inverse correlation matrix of inputs for learning recurrent weights\n",
    "        self.P = [\n",
    "            np.identity(len(self.W_plastic[i]))/self.delta \n",
    "            for i in range(self.N_plastic)\n",
    "        ]\n",
    "\n",
    "        # Output weights\n",
    "        self.W_out = (np.random.randn(self.No, self.N) / np.sqrt(self.N))\n",
    "\n",
    "        # Inverse correlation matrix of inputs for learning readout weights\n",
    "        self.P_out = [np.identity(self.N)/self.delta for i in range(self.No)]\n",
    "        \n",
    "    def reinitialize_readout_weights(self):\n",
    "        \"Reinitializes the readout weights while preserving the recurrent weights.\"\n",
    "\n",
    "        # Output weights\n",
    "        self.W_out = (np.random.randn(self.No, self.N) / np.sqrt(self.N))\n",
    "\n",
    "        # Inverse correlation matrix of inputs for learning readout weights\n",
    "        self.P_out = [np.identity(self.N)/self.delta for i in range(self.No)]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the activity in the network.\n",
    "        \"\"\"    \n",
    "        self.x = np.random.uniform(-1.0, 1.0, (self.N, 1))\n",
    "        self.r = np.tanh(self.x)\n",
    "        self.z = np.zeros((self.No, 1))\n",
    "\n",
    "    def step(self, I, noise=True):\n",
    "        \"\"\"\n",
    "        Updates neural variables for a single simulation step.\n",
    "        \n",
    "        * `I`: input at time t, numpy array of shape (Ni, 1)\n",
    "        * `noise`: if noise should be added to the recurrent neurons dynamics (should be False when recording the initial trajectory).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Noise can be shut off\n",
    "        I_noise = (self.Io * np.random.randn(self.N, 1) if noise else 0.0)\n",
    "        \n",
    "        # tau * dx/dt + x = I + W_rec * r + I_noise\n",
    "        self.x += (self.W_in @ I + self.W_rec @ self.r + I_noise - self.x)/self.tau\n",
    "        \n",
    "        # r = tanh(x)\n",
    "        self.r = np.tanh(self.x)\n",
    "        \n",
    "        # z = W_out * r\n",
    "        self.z = self.W_out @ self.r\n",
    "\n",
    "    def train_recurrent(self, target):\n",
    "        \"\"\"\n",
    "        Applies the RLS learning rule to the recurrent weights.\n",
    "        \n",
    "        * `target`: desired trajectory at time t, numpy array of shape (N, 1)\n",
    "        \"\"\"\n",
    "        # Compute the error of the recurrent neurons\n",
    "        error = target - self.r\n",
    "\n",
    "        # Apply the FORCE learning rule to the recurrent weights\n",
    "        for i in range(self.N_plastic): # for each plastic post neuron\n",
    "            \n",
    "            # Get the rates from the existing synapses only\n",
    "            r_plastic = self.r[self.W_plastic[i]]\n",
    "            \n",
    "            # Multiply the inverse correlation matrix with the rates\n",
    "            gain = self.P[i] @ r_plastic\n",
    "            \n",
    "            # Normalization term 1 + R'*P*R\n",
    "            normalization_term = (1. + r_plastic.T @  gain)\n",
    "            \n",
    "            # Update the inverse correlation matrix P <- P - ((P*R)*(P*R)')/(1+R'*P*R)\n",
    "            self.P[i] -= np.outer(gain, gain) / normalization_term\n",
    "\n",
    "            # Learning rule W <- W + e * (P*R)/(1+R'*P*R)\n",
    "            self.W_rec[i, self.W_plastic[i]] += (error[i] * gain / normalization_term)[:, 0]\n",
    "\n",
    "    def train_readout(self, target):\n",
    "        \"\"\"\n",
    "        Applies the RLS learning rule to the readout weights.\n",
    "        \n",
    "        * `target`: desired output at time t, numpy array of shape (No, 1)\n",
    "        \"\"\"\n",
    "        # Compute the error of the output neurons\n",
    "        error = target - self.z\n",
    "\n",
    "        # Apply the FORCE learning rule to the readout weights\n",
    "        for i in range(self.No): # for each readout neuron\n",
    "            \n",
    "            # Multiply the rates with the inverse correlation matrix P*R\n",
    "            gain = self.P_out[i] @ self.r\n",
    "            \n",
    "            # Normalization term 1 + R'*P*R\n",
    "            normalization_term = 1. + self.r.T @ gain\n",
    "            \n",
    "            # Update the inverse correlation matrix P <- P - ((P*R)*(P*R)')/(1+R'*P*R)\n",
    "            self.P_out[i] -= np.outer(gain, gain)/normalization_term\n",
    "            \n",
    "            # Learning rule W <- W - e * (P*R)/(1+R'*P*R)\n",
    "            self.W_out[i, :] += (error[i, 0] * gain / normalization_term)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the network with default parameters: 2 inputs, 800 neurons, 1 output. The scaling factor `g=1.5` ensures dynamics at the **edge of chaos**. The rest is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e1aad4ea-ddd1-423b-8c33-8053a182d49a"
   },
   "outputs": [],
   "source": [
    "Ni = 2 # Number of inputs\n",
    "N = 800 # Number of recurrent neurons\n",
    "No = 1 # Number of read-out neurons\n",
    "tau = 10.0 # Time constant of the neurons\n",
    "g = 1.5 # Synaptic strength scaling\n",
    "pc = 0.1 # Connection probability\n",
    "Io = 0.01 # Noise variance\n",
    "delta = 1.0 # Learning rate\n",
    "P_plastic = 0.6 # Percentage of neurons receiving plastic synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "794e5e46-c3e1-436b-9b7f-a04361b7838e"
   },
   "outputs": [],
   "source": [
    "net = LajeBuonomano(\n",
    "    Ni = Ni, # Number of inputs\n",
    "    N = N, # Number of recurrent neurons\n",
    "    No = No, # Number of read-out neurons\n",
    "    tau = tau, # Time constant of the neurons\n",
    "    g = g, # Synaptic strength scaling\n",
    "    pc = pc, # Connection probability\n",
    "    Io = Io, # Noise variance\n",
    "    delta = delta, # Learning rate\n",
    "    P_plastic = P_plastic, # Percentage of neurons receiving plastic synapses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Visualize the recurrent connectivity matrix `net.W_rec` with `plt.imshow`. For readability, only visualize the connections between the first 100 neurons or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(net.W_rec[:100, :100], cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a short impulse between 50 ms and 100 ms for one of the two input neurons, for a total duration of 1000 ms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9234259b-b70e-444a-b895-1d9fee3ecd21"
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "trial_duration = 1000\n",
    "t_start = 100\n",
    "\n",
    "impulse = np.zeros((trial_duration, Ni, 1))\n",
    "impulse[t_start - 50:t_start, 0, :] = 2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Plot the impulse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now provide the first impulse to the network by simulating 1000 time steps (=ms). We record the activity of the recurrent and readout neurons in numpy arrays: we will use this first run as a target for training later. Note that noise is disabled in this first run.\n",
    "The following cell displays the activity of the first 100 cells, as well as the detailed timecourse of some recurrent neurons and the readout neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9234259b-b70e-444a-b895-1d9fee3ecd21"
   },
   "outputs": [],
   "source": [
    "# Recording\n",
    "trajectory_target = []\n",
    "output_target = []\n",
    "\n",
    "# Reset the network\n",
    "net.reset()\n",
    "\n",
    "# Iterate over the trial duration\n",
    "for t in range(trial_duration):\n",
    "    net.step(impulse[t, :, :], noise=False)\n",
    "    trajectory_target.append(net.r)\n",
    "    output_target.append(net.z)\n",
    "    \n",
    "trajectory_target = np.array(trajectory_target)\n",
    "output_target = np.array(output_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = plt.subplot(131)\n",
    "ax.imshow(trajectory_target[:, :100, 0].T, aspect='auto', origin='lower', cmap=\"gray\")\n",
    "ax.set_title(\"Before\")\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Recurrent units')\n",
    "ax.set_xlabel('Initial run')\n",
    "\n",
    "ax = plt.subplot(132)\n",
    "ax.plot(trajectory_target[:, 0, 0] + 1, \"C0\")\n",
    "ax.plot(trajectory_target[:, 1, 0] + 3, \"C0\")\n",
    "ax.plot(trajectory_target[:, 2, 0] + 5, \"C0\")\n",
    "ax.set_title('Reservoir')\n",
    "ax.set_xlabel('Time (s)')\n",
    "\n",
    "ax = plt.subplot(133)\n",
    "ax.plot(output_target[:, 0, 0] + 1, \"C0\")\n",
    "ax.set_title('Readout')\n",
    "ax.set_xlabel('Time (s)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Do another trial, this time with `noise=True` after a reset. Do not overwrite `trajectory_target`. Make a plot that superposes the two runs (use the color `\"C1\"` for the second run). What do you observe? Do not hesitate to run longer simulations to see what happens, or to increase the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5386562e-e21f-4a2e-8093-05caebceef95"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's now train the recurrent weights to stabilize the trajectories. We will do 30 different trials with noise, with a call to `net.reset()` at the start of each trial. \n",
    "\n",
    "After each call to `step()` done after 100 ms (i.e. after the end of the impulse), you will call `net.train_recurrent(target=trajectory_target[t, :, :])` by passing the recorded activity at time $t$ during the first recorded trial. \n",
    "\n",
    "Beware: training is extremely expensive, as RLS requires matrix multiplications for each neuron. It is possible to call `net.train_recurrent()` only every 5 steps to save computations without impacting performance too much (`if t%5==0`). \n",
    "\n",
    "**Q:** Train the recurrent weights for 30 trials to reproduce the trajectory `trajectory_target`. After training, do a test trial and superpose it to `trajectory_target`. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a97d7b14-c33e-492d-ac31-a5fa3e2974f5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the readout weights. We first generate two target signals, a bump 500 ms after the end of the impulse, and a sine function. You can be creative and define other signals if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, trial_duration, trial_duration)\n",
    "\n",
    "# Bump target output\n",
    "bump = np.zeros((trial_duration, No, 1))\n",
    "bump[:, 0, 0] = 0.2 + 0.8 * np.exp(-(t - 600)**2/(30)**2)\n",
    "\n",
    "# Cosine target output\n",
    "cosine_fct = np.zeros((trial_duration, No, 1))\n",
    "cosine_fct[100:, 0, 0] = np.cos(t[100:]/20)\n",
    "cosine_fct[900:, 0, 0] = 0.\n",
    "\n",
    "# Select which target signal to use\n",
    "target = cosine_fct\n",
    "#target = bump\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(target[:, 0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cff9d348-30ce-4987-8827-4650c591e393"
   },
   "source": [
    "**Q:** Train the readout weights to reproduce one of the target signals for 10 trials. The only difference with the previous training phase is that you will call `net.train_readout()` at each time step. Do a test trial and visualize the output. Does it work? Reset the readout weights with `net.reinitialize_readout_weights()` and learn the other signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further demonstrate the robustness of the trained dynamics, let's perturb the reservoir by adding a small input to the second input neuron for a short period of time after the initial impulse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small perturbation at t = 300\n",
    "perturbation = impulse.copy() \n",
    "perturbation[300:310, 1, :] = 0.5 \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(perturbation[:, 0, 0])\n",
    "plt.plot(perturbation[:, 1, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Do a test trial using this input. Does the network recover its learned dynamics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "26084679-58d9-49ed-8e75-c7041f9149b5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now experiment freely with the network, for example:\n",
    "\n",
    "* Train the recurrent weights for less epochs and see if it is still robust to perturbations.\n",
    "* Try to learn different trajectories for different impulse signals and train a single readout neuron (or two?) to learn both target signals depending on the input.\n",
    "* Have two readout neurons reproduce trajectories in the 2D space, as the examples with \"chaos\" and \"neuron\" in the Laje & Buonomano paper.\n",
    "* Decrease or increase the number of neurons in the reservoir and see what happens to the dynamics.\n",
    "* Find a meaningful application of this network on the iCub simulator ;)"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "b2a03f00-db64-4cc0-8819-238df2008e1f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
